{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "454f106d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apex is not installed\n",
      "apex is not installed\n",
      "apex is not installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Fail to import ``MultiScaleDeformableAttention`` from ``mmcv.ops.multi_scale_deform_attn``, You should install ``mmcv-full`` if you need this module. \n",
      "FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "\n",
    "from mmpose.apis import init_pose_model\n",
    "from mmpose.datasets import DatasetInfo\n",
    "\n",
    "from video_inference_utils import infer_keypoint, apply_smoothnet_to_2d_seq, overlay_keypoints_on_video, output_keypoints_to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0267634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use load_from_local loader\n"
     ]
    }
   ],
   "source": [
    "# Initialize the models\n",
    "\n",
    "# YOLO model for human detection\n",
    "yolo_model = YOLO(\"weights/yolo11l.pt\")\n",
    "\n",
    "# Pose Estimation model for keypoints detection in human\n",
    "cfg_file = \"ViTPose/configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/ViTPose_large_coco_256x192.py\"\n",
    "ckpt_file = \"weights/vitpose-l.pth\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pose_model = init_pose_model(cfg_file, ckpt_file, device=device)\n",
    "dataset_name = pose_model.cfg.data.test.type\n",
    "dataset_info = DatasetInfo(pose_model.cfg.data.test.dataset_info)\n",
    "\n",
    "# SmoothNet model for smoothening the keypoints predictions between frames\n",
    "smoothnet_config = 'SmoothNet/configs/h36m_fcn_3D.yaml'\n",
    "smoothnet_checkpoint = 'weights/checkpoint_32.pth.tar'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201680fb",
   "metadata": {},
   "source": [
    "### Video inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df4e48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish inferece on shoplifting1.MP4, \n",
      "please go ahead to output the inferred keypoint results and video.\n"
     ]
    }
   ],
   "source": [
    "video_path = 'video_clips/shoplifting1.MP4' # enter the video input path\n",
    "output_dir = 'video_output'\n",
    "\n",
    "os.makedirs(output_dir,exist_ok=True)\n",
    "output_path = os.path.join(output_dir,video_path.split('/')[-1].split('.')[0]+'_track.mp4')\n",
    "\n",
    "# get video information\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# configuration of the tracker\n",
    "tracker_yaml=r'bytetrack.yaml'\n",
    "\n",
    "track_sessions = {} # {tracker_id: sessions}\n",
    "# where sessions = {} with key: frame_ind_start, item: (frame_ind_end, smoothened_kpts)\n",
    "\n",
    "track_curr_start = {} # {tracker_id: frame_ind_start}\n",
    "\n",
    "track_kpts ={} # {tracker_id: kpts}\n",
    "# where kpts = [] with shape = (T,J,2) \n",
    "#  T: # frames, J: #  keypoints\n",
    "# '2': for for 'x' and 'y' in the dimension of [x,y] corrdinate\n",
    "\n",
    "track_patience = {} # {tracker_id: patience}\n",
    "patience0 = 4\n",
    "\n",
    "\n",
    "# YOLO inference with tracking and Keypoints inference by ViTPose (Smoothened by SmoothNet)\n",
    "frame_ind = 0 \n",
    "for result in yolo_model.track(source=video_path, tracker=tracker_yaml, conf=0.5,iou=0.65,stream=True,device=device,verbose=False):\n",
    "    \n",
    "    frame = result.orig_img\n",
    "    detections = sv.Detections.from_ultralytics(result)\n",
    "\n",
    "    if result.boxes.id is not None:\n",
    "        \n",
    "        detections.tracker_id=result.boxes.id.cpu().numpy().astype(int)\n",
    "        detections = detections[detections.class_id == 0] # get only the person class detections\n",
    "\n",
    "        pose_results = infer_keypoint(frame, detections, pose_model=pose_model, dataset_name=dataset_name, dataset_info=dataset_info)\n",
    "\n",
    "        for ind in range(len(detections)):\n",
    "            tracker_id = detections.tracker_id[ind]\n",
    "\n",
    "            if tracker_id not in track_sessions:\n",
    "                track_sessions[tracker_id] = {}\n",
    "                track_curr_start[tracker_id] = None\n",
    "                track_kpts[tracker_id] = []\n",
    "                track_patience[tracker_id] = patience0\n",
    "            \n",
    "            kpts_2d = pose_results[ind][\"keypoints\"][:, :2]  # (J, 2)\n",
    "            track_kpts[tracker_id].append(kpts_2d)\n",
    "            if track_curr_start[tracker_id] is None: track_curr_start[tracker_id] = frame_ind\n",
    "            if track_patience[tracker_id] < patience0: track_patience[tracker_id] = patience0\n",
    "\n",
    "        tracker_ids = set(detections.tracker_id)\n",
    "    else:\n",
    "        tracker_ids = set()\n",
    "    \n",
    "    for tracker_id in track_sessions:\n",
    "\n",
    "        if tracker_id not in tracker_ids and track_kpts[tracker_id]:\n",
    "\n",
    "            if track_patience[tracker_id]>0:\n",
    "\n",
    "                track_patience[tracker_id] -= 1\n",
    "                track_kpts[tracker_id].append(track_kpts[tracker_id][-1])\n",
    "\n",
    "            elif track_patience[tracker_id]==0:\n",
    "\n",
    "                all_kpts = np.stack(track_kpts[tracker_id], axis=0)  # (T, J, 2)\n",
    "\n",
    "                smoothened_kpts = apply_smoothnet_to_2d_seq(\n",
    "                    all_kpts,\n",
    "                    smoothnet_config,\n",
    "                    smoothnet_checkpoint,\n",
    "                    image_shape=(height, width)\n",
    "                )\n",
    "                frame_ind_start = track_curr_start[tracker_id]\n",
    "                track_sessions[tracker_id][frame_ind_start] = (frame_ind-1,smoothened_kpts)\n",
    "                \n",
    "                # empty track_kpts[tracker_id] \n",
    "                # and reset track_curr_start and patience for the new session\n",
    "                track_kpts[tracker_id] = [] \n",
    "                track_curr_start[tracker_id] = None\n",
    "                track_patience[tracker_id] = patience0\n",
    "    \n",
    "    frame_ind+=1\n",
    "\n",
    "for tracker_id in track_sessions:\n",
    "\n",
    "    if track_kpts[tracker_id]:\n",
    "\n",
    "        all_kpts = np.stack(track_kpts[tracker_id], axis=0)  # (T, J, 2)\n",
    "\n",
    "        smoothened_kpts = apply_smoothnet_to_2d_seq(\n",
    "            all_kpts,\n",
    "            smoothnet_config,\n",
    "            smoothnet_checkpoint,\n",
    "            image_shape=(height, width)\n",
    "        )\n",
    "        frame_ind_start = track_curr_start[tracker_id]\n",
    "        track_sessions[tracker_id][frame_ind_start] = (frame_ind-1,smoothened_kpts)\n",
    "\n",
    "cap.release()\n",
    "\n",
    "print('Finished inference on {}, \\nplease go ahead to output the inferred keypoint results and video.'.format(\n",
    "    video_path.split('/')[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37671c2",
   "metadata": {},
   "source": [
    "### Draw the output video and save the inferred keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe1cb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved video with smoothed keypoints to: video_output\\shoplifting1_track.mp4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================== Drawing the keypoints for each tracked person in the video =====================\n",
    "\n",
    "# # if want to load the save keypoints from previous sessions, uncomment the following few lines...\n",
    "# npz_path = 'video_output\\\\Normal_Videos314_x264.npz'# change the npz path here for loading the file\n",
    "# loaded = np.load(npz_path,allow_pickle=True)\n",
    "# video_path = loaded['video_path'].flatten()[0]\n",
    "# track_sessions = loaded['track_sessions'].flatten()[0]\n",
    "# output_path = loaded['output_path'].flatten()[0]\n",
    "\n",
    "colors = [\n",
    "    (0, 255, 0),       # Green\n",
    "    (255, 165, 0),     # Orange\n",
    "    (0, 0, 255),       # Blue\n",
    "    (0, 255, 255),     # Cyan\n",
    "    (255, 255, 0),     # Yellow\n",
    "    (255, 0, 255),     # Magenta\n",
    "    (255, 0, 0),       # Red\n",
    "    (0, 0, 0),         # Black\n",
    "    (255, 255, 255),   # White\n",
    "    (128, 128, 128),   # Gray\n",
    "] # rgb\n",
    "color_map = {cc: colors[color_i] for color_i, cc in enumerate(list(track_sessions.keys()))} # {tracker_id: color}\n",
    "overlay_keypoints_on_video(\n",
    "    video_path=video_path,\n",
    "    track_sessions=track_sessions,\n",
    "    output_path=output_path,\n",
    "    color_map=color_map\n",
    ")\n",
    "\n",
    "# ===================== Save the smoothened keypoints for plotting in videos ===================== \n",
    "\n",
    "video_name = video_path.split('/')[-1].split('.')[0]\n",
    "npz_path = os.path.join(output_dir,video_name+'.npz')\n",
    "np.savez(npz_path,video_path=video_path,track_sessions=track_sessions,output_path=output_path)\n",
    "\n",
    "# ===================== Create the json file for the inference by STG-NF model ===================== \n",
    "\n",
    "video_name_map = {\n",
    "    'Normal_Videos313_x264':'07',\n",
    "    'Normal_Videos314_x264':'08',\n",
    "    'shoplifting1':'09',\n",
    "    'shoplifting2':'10'\n",
    "} # to match the STG-NF model format\n",
    "\n",
    "json_dir = os.path.join(output_dir,video_name,video_name_map[video_name])\n",
    "os.makedirs(os.path.join(output_dir,video_name),exist_ok=True)\n",
    "output_keypoints_to_json(track_sessions, output_dir=json_dir)\n",
    "\n",
    "# copy the json files to the STG-NF data directory\n",
    "jsons = [os.path.join(output_dir,video_name,js) for js in os.listdir(os.path.join(output_dir,video_name)) if js.endswith('.json')]\n",
    "for js in jsons:\n",
    "    shutil.copy(js,os.path.join('STG_NF','data','PoseLift','pose','test'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76fc06c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fc92dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STG-NF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
